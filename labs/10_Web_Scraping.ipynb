{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![What is Html](http://designshack.designshack.netdna-cdn.com/wp-content/uploads/htmlbasics-0.jpg)\n",
    "\n",
    "One of the largest sources of data in the world, is all around us.  We consume the web in some form, every day.  One of the most powerful tools you can learn, allows to to extract and normalize data from undstructured sources.  If you can see it, it can be scraped, mined, and put into a dataframe.\n",
    "\n",
    "Today, we will walk through some basic constructs that describe HTML as unstructured data, a power selection technique called XPath, and a basic workflow from a framework called [Scrapy](http://www.scrapy.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HTML\n",
    "\n",
    "In the HTML DOM (Document Object Model), everything is a node:\n",
    " * The document itself is a document node.\n",
    " * All HTML elements are element nodes.\n",
    " * All HTML attributes are attribute nodes.\n",
    " * Text inside HTML elements are text nodes.\n",
    " * Comments are comment nodes.\n",
    "\n",
    "## Elements\n",
    "Elements begin and end with open and close \"tags\", which are defined by namespaced, encapsulated strings. \n",
    "\n",
    "```\n",
    "<title>I am a title.</title>\n",
    "<p>I am a paragraph.</p>\n",
    "<strong>I am bold.</strong>\n",
    "```\n",
    "\n",
    "**Elements begin and end in the same namespace like so:**  `<p></p>`\n",
    "\n",
    "**Elements can have parents and children:**\n",
    "\n",
    "```\n",
    "<body>\n",
    "    <div>I am inside the parent element\n",
    "        <div>I am inside a child element</div>\n",
    "        <div>I am inside another child element</div>\n",
    "        <div>I am inside yet another child element</div>\n",
    "    </div>\n",
    "</body>\n",
    "```\n",
    "\n",
    "## Attributes\n",
    "HTML elements can have attributes.  They describe properties, and characteristics of elements.  Some affect how the element behaves or looks in terms of the rendered output by the browser.\n",
    "\n",
    "The most common element is an \"anchor\" element.  Anchor elements typically have an \"href\" element, which tells the browser where to go after it is clicked.  Anchor elements typically are formatted in bold, and sometimes are underlined as a visual cue to differentiate itself.\n",
    "\n",
    "**Markup that describes nn element with attributes, litterally looks like this**\n",
    "\n",
    "```\n",
    "<a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">An Awesome Website</a>\n",
    "```\n",
    "\n",
    "**However, this element, once rendered, looks like this**\n",
    "\n",
    "[An Awesome Website](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\n",
    "\n",
    "## Element Heirarchy\n",
    "\n",
    "![Nodes](http://www.computerhope.com/jargon/d/dom1.jpg)\n",
    "\n",
    "**Literally Represented:**\n",
    "\n",
    "```\n",
    "<html>\n",
    "    \n",
    "    <head>\n",
    "        <title>Example</title>\n",
    "    </head>\n",
    "    \n",
    "    <body>\n",
    "        <h1>Example Page</h1>\n",
    "        <p>This is an example page.</p>\n",
    "    </body>\n",
    "    \n",
    "</html>\n",
    "```\n",
    "## You are now qualified HTML experts\n",
    "\n",
    "![](http://hpcc.advancingexpertcare.org/wp-content/uploads/2014/10/certified.jpg)\n",
    "\n",
    "Your HTML learning can continue...\n",
    "\n",
    "Read all about the different elements supported amongst modern browsers:\n",
    " * [HTML5 Cheatsheet](http://websitesetup.org/html5-cheat-sheet/)\n",
    " * [Mozilla HTML Element Reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    " * [HTML5 Visual Cheatsheet](http://www.unitedleather.biz/PDF/HTML5-Visual-Cheat-Sheet1.pdf)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# What is XPath?\n",
    "\n",
    "<img src=\"http://img.crx4chrome.com/63/4c/b1/hgimnogjllphhhkhlmebbmlgjoejdpjl-screenshot.jpg\">\n",
    "\n",
    "Now that we're all familiar with hypertext markup language (HTML), understanding how to identify elements and attributes within HTML documents, gives us the capability to write simple expressions that create structured data.\n",
    "\n",
    "To make this process easier to deal with, we will be using XPath helper, which is a Chrome addon.  It's not necessary, but highly recommended, to help build XPath expressions.\n",
    "\n",
    "[XPath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en)\n",
    "\n",
    "\n",
    "\n",
    "## Multiple vs Singlular Selections\n",
    "XPath expressions can select elements, element attributes, and element text.  These selections can be either to a single item, or multiple items.  Generally, if you're not specific enough, you will end up selecting multiple elements.\n",
    "\n",
    "***Multiple selections*** are useful for capturing search results, or any repeating element.  For instance, the _titles_ of an apartment listing search results from Craigslist:\n",
    "\n",
    "**URL**\n",
    "\n",
    "[http://sfbay.craigslist.org/search/sfc/apa](http://sfbay.craigslist.org/search/sfc/apa)\n",
    "\n",
    "\n",
    "**HTML Markup**\n",
    "```\n",
    "...\n",
    "<span class=\"pl\"> \n",
    "    <time datetime=\"2016-01-12 23:27\" title=\"Tue 12 Jan 11:27:35 PM\">Jan 12</time> \n",
    "    <a href=\"/sfc/apa/5400584579.html\" data-id=\"5400584579\" class=\"hdrlnk\">Welcome home to a sweetly renovated four bedroom one and a half bath</a> \n",
    "</span>\n",
    "...\n",
    "```\n",
    "\n",
    "**XPath - Multiple Titles**\n",
    "```\n",
    "//a[@class='hdrlnk']\n",
    "```\n",
    "\n",
    "**Returns (Ad Titles)**\n",
    "```\n",
    "***New Remodeled two bedroom Apartment***\n",
    "WONDERFUL ONE BR APARTMENT HOME\n",
    "Beautiful 1bed/1bath Apartment in Russian Hill NO SECURITY DEPOSIT\n",
    "Knockout SF View|Green Oasis|Private Driveway|Furnished\n",
    "3BR/3BA Spacious, Beautiful SOMA Loft: 5 month lease\n",
    "Nob Hill Large Studio - Light, Quiet, Lovely Building\n",
    "etc...\n",
    "```\n",
    "\n",
    "<br><br><br>\n",
    "***Singular selections*** are necessary when you want to grab specific, unique text within elements.  Here's an example (which is probably going to be expired if you view it sometime after Jan 12th, 2016) of a details page on Craigslist:\n",
    "\n",
    "**URL**\n",
    "(Only $8000!)\n",
    "[http://sfbay.craigslist.org/sfc/apa/5400585892.html](http://sfbay.craigslist.org/sfc/apa/5400585892.html)\n",
    "\n",
    "**HTML Markup**\n",
    "```\n",
    "<div class=\"postinginfos\">\n",
    "    <p class=\"postinginfo\">post id: 5400585892</p>\n",
    "    <p class=\"postinginfo\">posted: <time datetime=\"2016-01-12T23:23:19-0800\" class=\"xh-highlight\">2016-01-12 11:23pm</time></p>\n",
    "    <p class=\"postinginfo\"><a href=\"https://accounts.craigslist.org/eaf?postingID=5400585892\" class=\"tsb\">email to friend</a></p>\n",
    "    <p class=\"postinginfo\"><a class=\"bestof-link\" data-flag=\"9\" href=\"https://post.craigslist.org/flag?flagCode=9&amp;postingID=5400585892\" title=\"nominate for best-of-CL\"><span class=\"bestof-icon\">♥ </span><span class=\"bestof-text\">best of</span></a> <sup>[<a href=\"http://www.craigslist.org/about/best-of-craigslist\">?</a>]</sup>    </p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "**XPath - Single Item**\n",
    "```\n",
    "//p[@class='postinginfo'][2]/time\n",
    "```\n",
    "**Returns (Time of posting)**\n",
    "```\n",
    "2016-01-12 11:23pm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is [Scrapy](http://scrapy.org/)?\n",
    "\n",
    "_\"Scrapy is an application framework for writing web spiders that crawl web sites and extract data from them.\"_\n",
    "\n",
    "## Installation\n",
    "\n",
    "```\n",
    "pip install scrapy\n",
    "```\n",
    "\n",
    "_*note: Scrapy does not work with Python 3!!!_\n",
    "\n",
    "### Windows Users\n",
    "\n",
    "If Scrapy crashes with:\n",
    "\n",
    ">`ImportError: No module named win32api`\n",
    "\n",
    "You need to install [pywin32](http://sourceforge.net/projects/pywin32/) because of [this Twisted bug](http://twistedmatrix.com/trac/ticket/3707).\n",
    "\n",
    "\n",
    "## 1. Create a new Scrapy project\n",
    "\n",
    "\n",
    "> `scrapy startproject craigslist`\n",
    "\n",
    "Should create output that looks like this:\n",
    "<blockquote>\n",
    "```\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
    "2016-01-13 00:12:45 [scrapy] INFO: Overridden settings: {}\n",
    "New Scrapy project 'craigslist' created in:\n",
    "    /Users/davidyerrington/virtualenvs/data/scraping/craigslist\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd craigslist\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "This command generates a set of project files:\n",
    "<blockquote>\n",
    "```\n",
    "craigslist/\n",
    "    scrapy.cfg\n",
    "    craigslist/\n",
    "        __init__.py\n",
    "        items.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "            __init__.py\n",
    "            ...\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "Generally, these are our files.  We will go into more depth about these soon.\n",
    "\n",
    " * **scrapy.cfg:** the project configuration file\n",
    " * **craigslist/:** the project’s python module, you’ll later import your code from here.\n",
    " * **craigslist/items.py:** the project’s items file.\n",
    " * **craigslist/pipelines.py:** the project’s pipelines file.\n",
    " * **craigslist/settings.py:** the project’s settings file.\n",
    " * **craigslist/spiders/:** a directory where you’ll later put your spiders.\n",
    " \n",
    "Long story, but please add this line to your craigslist/settings.py file before continuing:\n",
    " \n",
    " <blockquote>\n",
    " ```\n",
    " DOWNLOAD_HANDLERS = {'s3': None,}\n",
    " ```\n",
    " </blockquote>\n",
    "\n",
    "## 2. Define an \"Item\"\n",
    "\n",
    "Basically, when we define an item, it's telling our new application what it will be collecting.  In essence, an \"item\", is an entity that has attributes (ie: \"title\", \"description\", \"price\", etc) that are descriptive and relate to elements on pages that we will be scraping.  \n",
    "\n",
    "In more precise terms, this is a model, for those who are familliar with ORM, or relational database terms.  Don't worry if this is a foreign concept.  The main idea to undersatnd is that a model has attributes that closely resemble / relate to elements on our target web page(s).\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class CraigslistItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "## 3. A Spider That Crawls\n",
    "\n",
    "An item is a model that resembles data on a webpage.  A spider is something that crawls, and uses our item model, to old our items for us.\n",
    "\n",
    "Scrapy spiders are classes.  Let's write our first file, called `craigslist_spider.py` and put it in our `/spiders` directory:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "import scrapy\n",
    "\n",
    "class CraigslistSpider(scrapy.Spider):\n",
    "    name = \"craigslist\"\n",
    "    allowed_domains = [\"craigslist.org\"]\n",
    "    start_urls = [\n",
    "        \"http://sfbay.craigslist.org/search/sfc/apa\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        filename = response.url.split(\"/\")[-2]\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "Let's dive in and crawl from our `/craigslist/craigslist` directory:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "scrapy crawl craigslist\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "What just happened?\n",
    "\n",
    " * Our application requested the URLs from the `start_urls` class attribute.\n",
    " * Ran parse over the content containing the HTML markup, of each request URL.\n",
    " * What else?\n",
    " \n",
    "<blockquote>\n",
    "```\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "It saved a file in our base project directory.  It shoudl be named based on the end of the URL.  In our case, it should create a file called \"sfc\".  This is taken directly from the Scrapy docs and it's only point is to illustrate teh workflow so far.  It is kind of nice to have a reference to our HTML file though.  \n",
    "\n",
    "There might be some errors listed when we crawl, but they are fine for now.\n",
    "\n",
    "## 4. XPath + Parsing /w Our Spider\n",
    "\n",
    "So far, we've defined what fields we'll get, some urls to fetch, and saved some content to a file.  Let's actually do something interesting.\n",
    "\n",
    "We should let our spider know about our item model we made earlier.  In the head of the `craigslist/craigslist/spiders/craigslist_spider.py`, lets add a new import:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "from craigslist.items import CraigslistItem\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "Why won't it work otherwise?\n",
    "\n",
    "<br><br><br>\n",
    "Let's replace our parse method, to find some data from our Craigslist spider response, and map it to our item model, CraigslistItem.\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "    def parse(self, response):\n",
    "\n",
    "        for sel in response.xpath(\"//div[@class='content']/span[@class='rows']/p\"):\n",
    "\n",
    "            item = CraigslistItem()\n",
    "            item['title'] =  sel.xpath(\"span/span/a[@class='hdrlnk']\").extract()[0]\n",
    "            item['link']  =  sel.xpath(\"span/span/a[@class='hdrlnk']/@href\").extract()[0]\n",
    "            yield item\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save our data\n",
    "By default, we can save our crawled data as json.  To save our data, we just need to pass an optional parameter to our crawl call:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "scrapy crawl craigslist -o items.json\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "### Let's Look at Our Data\n",
    "It's always good to iteratively check our data when developing a spider, to make sure it's close to what we want. \n",
    "\n",
    "_Pro tip:  The longer your iterations are between checks, the harder it's going to be to understand what's no working and fix bugs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/sfc/apa/5401845169.html</td>\n",
       "      <td>&lt;span class=\"price\"&gt;$5250&lt;/span&gt;</td>\n",
       "      <td>&lt;a href=\"/sfc/apa/5401845169.html\" data-id=\"54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/sfc/apa/5401844876.html</td>\n",
       "      <td>&lt;span class=\"price\"&gt;$3995&lt;/span&gt;</td>\n",
       "      <td>&lt;a href=\"/sfc/apa/5401844876.html\" data-id=\"54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/sfc/apa/5401832595.html</td>\n",
       "      <td>&lt;span class=\"price\"&gt;$3800&lt;/span&gt;</td>\n",
       "      <td>&lt;a href=\"/sfc/apa/5401832595.html\" data-id=\"54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/sfc/apa/5401842161.html</td>\n",
       "      <td>&lt;span class=\"price\"&gt;$3650&lt;/span&gt;</td>\n",
       "      <td>&lt;a href=\"/sfc/apa/5401842161.html\" data-id=\"54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/sfc/apa/5401841489.html</td>\n",
       "      <td>&lt;span class=\"price\"&gt;$3750&lt;/span&gt;</td>\n",
       "      <td>&lt;a href=\"/sfc/apa/5401841489.html\" data-id=\"54...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       link                             price  \\\n",
       "0  /sfc/apa/5401845169.html  <span class=\"price\">$5250</span>   \n",
       "1  /sfc/apa/5401844876.html  <span class=\"price\">$3995</span>   \n",
       "2  /sfc/apa/5401832595.html  <span class=\"price\">$3800</span>   \n",
       "3  /sfc/apa/5401842161.html  <span class=\"price\">$3650</span>   \n",
       "4  /sfc/apa/5401841489.html  <span class=\"price\">$3750</span>   \n",
       "\n",
       "                                               title  \n",
       "0  <a href=\"/sfc/apa/5401845169.html\" data-id=\"54...  \n",
       "1  <a href=\"/sfc/apa/5401844876.html\" data-id=\"54...  \n",
       "2  <a href=\"/sfc/apa/5401832595.html\" data-id=\"54...  \n",
       "3  <a href=\"/sfc/apa/5401842161.html\" data-id=\"54...  \n",
       "4  <a href=\"/sfc/apa/5401841489.html\" data-id=\"54...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# update this path to your own\n",
    "# hint: from terminal, use the pwd command in the same directory as items.json to find\n",
    "# your scraping directory with your json file\n",
    "pd.read_json(\"/Users/davidyerrington/virtualenvs/data/scraping/craigslist/craigslist/items.json\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's wrong with this data so far?\n",
    "\n",
    "### 1)  We need to update the way we select our \"title\" attribute.  Google this and try to fix the problem.  This is what we do all day as developers.\n",
    "\n",
    "**Discus and fix**...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Now, we would like to add the price attribute into our items.  What files need to be updated?  What needs to be added to our spider?  \n",
    "\n",
    "**Update our code!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the workflow that is useful in this context, is to load the page in your Chrome browser, check out the page using the XPath Helper plugin, then deriving your own XPath expressions from the output.\n",
    "\n",
    "Generally, `text()` selects only the text of a given element (between the tags), and `@attribute_name` is used to select attributes.\n",
    "\n",
    "**Here are a few examples**\n",
    "\n",
    "### Text()\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "<h1>Darwin - The Evolution Of An Exhibition</h1>\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "The XPath selector for this:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "//h1/text()\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "### @Attributes\n",
    "\n",
    "And the description is contained inside a `<div>` tag with `id=\"description\"`:\n",
    "<blockquote>\n",
    "```\n",
    "<h2>Description:</h2>\n",
    "\n",
    "<div id=\"description\">\n",
    "Short documentary made for Plymouth City Museum and Art Gallery regarding the setup of an exhibit about Charles Darwin in conjunction with the 200th anniversary of his birth.\n",
    "</div>\n",
    "...\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "XPath\n",
    "<blockquote>\n",
    "```\n",
    "//div[@id='description']\n",
    "```\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting More Results\n",
    "\n",
    "100 results is pretty cool but what if we want more?  We need to follow the \"next\" links, and find new pages to grab.  Using the **parse()** method of our spider class, we only need to return another type of object.\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "    def parse(self, response):\n",
    "\n",
    "        for sel in response.xpath(\"//div[@class='content']/span[@class='rows']/p\"):\n",
    "\n",
    "            item = CraigslistItem()\n",
    "            item['title'] =  sel.xpath(\"span/span/a[@class='hdrlnk']\").extract()[0]\n",
    "            item['link']  =  sel.xpath(\"span/span/a[@class='hdrlnk']/@href\").extract()[0]\n",
    "            item['price'] =  sel.xpath(\"span/span/span[@class='price']\").extract()[0]\n",
    "            yield item\n",
    "\n",
    "        # Does the next page exist?  Let's get it!\n",
    "        next_page   = response.xpath(\"(//a[@class='button next']/@href)[1]\")\n",
    "\n",
    "        if next_page:\n",
    "            url = response.urljoin(next_page[0].extract())\n",
    "            yield scrapy.Request(url, self.parse)\n",
    "\n",
    "```\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "Loading up an interactive \"scrapy shell\" will help make testing your XPath expressions even easier.\n",
    "\n",
    "To start the shell, you just need to run this command:\n",
    "\n",
    "<blockquote>\n",
    "```\n",
    "scrapy shell http://sfbay.craigslist.org/search/apa\n",
    "```\n",
    "</blockquote>\n",
    "\n",
    "Learn more about it:  [Scrapy Shell Documentation](http://doc.scrapy.org/en/0.24/topics/shell.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
