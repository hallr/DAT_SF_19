{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT19 Lab 13\n",
    "## Ensemble Methods: Aggregating Models\n",
    "\n",
    "\n",
    "Today's lab covers:\n",
    "1. Investigating the Decision Boundaries\n",
    "2. Growing a Random Forest\n",
    "\n",
    "The theory used to introduce these concepts can be a bit high level. We postulate a theoretical *true* classification **f** function that perfectly labels a data point given a large enough set of input features. We try to come as close as possible to this **f** with our learned classifier **h**. \n",
    "\n",
    "The concept of our *hypothesis space* is what we explore when we take our data, learn a model, and determine the \"score\" of our model. In theory, a cross validated score of 100 is as close as we can get to **f** and all these improvements we make to our classifiers are moving through the hypothesis space, approaching **f**.\n",
    "\n",
    "This theoretical component has no true lab but in a sense is every lab that we've covered. Whenever we compare the performance of two separate models, we are comparing the difference from two distinct starting points in the hypothesis space.\n",
    "\n",
    "A way to demonstrate this will be the basis of the first part of our lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis  as QDA\n",
    "\n",
    "from bokeh.plotting import figure,gridplot,show,output_notebook\n",
    "from bokeh.models import Range1d\n",
    "output_notebook()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Investigating Decision Boundaries\n",
    "Visually understanding how some classifiers make decisions; adapted from an sklearn page, which will be shared after lab. \n",
    "\n",
    "The goal here is to investigate the code and describe in layman's terms what is happening in the specific chunk. If there is a variable being referenced outside of your chunk, or your in a loop, refer to what's happening in one iteration of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# ====================================================================== #\n",
    "# Part 1\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\",\n",
    "         \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"LDA\", \"QDA\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LDA(),\n",
    "    QDA()]\n",
    "\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe here what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_grid = []\n",
    "for data in datasets:\n",
    "    \n",
    "# NOTE THIS IS A FOR LOOP, so decribe what happens on one iteration\n",
    "# ====================================================================== #\n",
    "# Part 2\n",
    "\n",
    "    h = .02  # step size in the meshgrid\n",
    "    \n",
    "    X, y = data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "# ====================================================================== #\n",
    "    # Part 3\n",
    "    raw = figure(title=\"Raw Data\", tools='',\n",
    "                x_range=(xx.min(), xx.max()), \n",
    "                y_range=(yy.min(), yy.max()),\n",
    "                width=180, plot_height=220,\n",
    "                min_border=1,\n",
    "                title_text_font_size='10pt' )\n",
    "\n",
    "    color_dict = {0:'blue',1:'red'}\n",
    "\n",
    "    train_colors = [color_dict[label] for label in y_train]\n",
    "    raw.circle(X_train[:, 0], X_train[:, 1], \n",
    "               color=train_colors)\n",
    "\n",
    "    test_colors = [color_dict[label] for label in y_test]\n",
    "    raw.circle(X_test[:, 0], X_test[:, 1], \n",
    "               color=test_colors, alpha=0.6)\n",
    "\n",
    "\n",
    "    \n",
    "# ====================================================================== #\n",
    "# Part 4\n",
    "\n",
    "    row=[raw]\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "        \n",
    "        Z = Z.reshape(xx.shape)\n",
    "        Z = Z/Z.max()\n",
    "        control_row = np.ones(Z.shape[1])+0.001\n",
    "        #NOTE: control row adds a max value outside of the grid to correct colors. \n",
    "        # Don't worry about explaining control_row\n",
    "        Z = np.vstack((Z,control_row))\n",
    "\n",
    "\n",
    "# ====================================================================== #\n",
    "# Part 5\n",
    "        p1 = figure(title=name, tools='',\n",
    "                    x_range=(xx.min(), xx.max()), \n",
    "                    y_range=(yy.min(), yy.max()),\n",
    "                    width=180, plot_height=220,\n",
    "                    min_border=1,\n",
    "                    title_text_font_size='10pt')\n",
    "\n",
    "\n",
    "        p1.image(image=[Z], x=[xx.min()], y=[yy.min()], dw=[xx.max()-xx.min()], dh=[yy.max()-yy.min()],\n",
    "                palette='RdYlBu11')\n",
    "\n",
    "        p1.circle(X_train[:, 0], X_train[:, 1], \n",
    "                  color=train_colors,\n",
    "                  line_color='black',\n",
    "                  size=3)\n",
    "        p1.circle(X_test[:, 0], X_test[:, 1], \n",
    "                  color=test_colors, \n",
    "                  alpha=0.6,\n",
    "                  line_color='black',\n",
    "                  size=3)\n",
    "\n",
    "        p1.text(x=[xx.max()-1.5],y=[yy.min()],\n",
    "                text=[str(score)],\n",
    "                text_font_size='15pt',\n",
    "                text_font_style='bold')\n",
    "        \n",
    "        row.append(p1)\n",
    "    \n",
    "    grid = np.array(row).reshape(2,len(row)/2)\n",
    "    full_grid.append(grid)\n",
    "    \n",
    "final_grid = np.vstack(full_grid)\n",
    "show(gridplot(final_grid.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "Random forests are a type of ensemble method (which we hinted at above) that build out groups of decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "titanic = pd.read_csv('../data/titanic.csv')\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preprocess our data. Age needs to be imputed or dropped, Sex needs to be converted to a boolean data type, and to use our categorical feature `Embarked` we must use `pd.get_dummies` on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_age = titanic['Age'].mean()\n",
    "titanic['Age'] = titanic['Age'].fillna(average_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic['is_female'] = titanic.Sex=='female'\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embarked_categories = pd.get_dummies(titanic.Embarked,prefix='Embarked')\n",
    "titanic[embarked_categories.columns] = embarked_categories\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_columns = 'Age,Parch,Pclass,Fare,Embarked_C,Embarked_Q,Embarked_S'.split(',')\n",
    "for train,test in ShuffleSplit(len(titanic),test_size=.2,n_iter = 1):\n",
    "\n",
    "    X_train = titanic[feature_columns].iloc[train]\n",
    "    y_train = titanic['Survived'].iloc[train]\n",
    "    X_test = titanic[feature_columns].iloc[test]\n",
    "    y_test = titanic['Survived'].iloc[test]\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We already imported the RandomForest above, but here it is for reference:\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=5, n_estimators=100, max_features=3)\n",
    "rfc.fit(X_train,y_train)\n",
    "rfc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting general score but let's be more thorough and get a cross evaluation score, and see how it grows with the size of our forest (`n_estimators`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "features = titanic[feature_columns]\n",
    "target = titanic['Survived']\n",
    "\n",
    "forest_size = range(2,100,2)\n",
    "y = []\n",
    "for i in forest_size:\n",
    "    rfc = RandomForestClassifier(max_depth=5, n_estimators=i, max_features=4)\n",
    "    accuracy = cross_val_score(rfc, features, target, cv=5).mean()\n",
    "    \n",
    "    y.append(accuracy)\n",
    "p1 = figure(title='Random Forest Classifier performance',tools='')\n",
    "p1.line(x=forest_size,y=y, color='green')\n",
    "\n",
    "\n",
    "show(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the classifier change as we allow for more complexity?  (i.e. deeper trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_depth = range(1,50)\n",
    "y = []\n",
    "for i in tree_depth:\n",
    "    rfc = RandomForestClassifier(max_depth=i, n_estimators=20, max_features=4)\n",
    "    accuracy = cross_val_score(rfc, features, target, cv=5).mean()\n",
    "    \n",
    "    y.append(accuracy)\n",
    "p1 = figure(title='Random Forest Classifier performance',x_axis_label='Max Tree Depth',tools='')\n",
    "p1.line(x=tree_depth,y=y, color='green')\n",
    "\n",
    "\n",
    "show(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise: Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cell_phone_churn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some Preprocessing\n",
    "state_encoder = LabelEncoder()  \n",
    "df.state = state_encoder.fit_transform(df.state)\n",
    "\n",
    "\n",
    "binary_columns = ['intl_plan', 'vmail_plan']  \n",
    "for col in binary_columns:  \n",
    "    df[col] = df[col].map({\n",
    "            'no': 0,\n",
    "            'yes': 1\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "----\n",
    "Explore the data and plot histograms of several columns. Hypothesize whether a relationship exists. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get you started:\n",
    "df['churn'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate and compare the `precision and recall` of \n",
    "    - `GradientBoostingClassifier` \n",
    "    - `RandomForestClassifier`\n",
    "    - `DecisionTreeClassifier`\n",
    "    - `LogisticRegression`\n",
    "        - hint!: use the `precision_recall_fscore_support` built into sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "help(precision_recall_fscore_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the models using a grid-search of several parameters \n",
    "    - Determine which score (accuracy? F1 score? ...) you will use for your grid search. Explain why to your partner\n",
    "    - Hint: for `DecisionTreeClassifier` and `RandomForestClassifier`, \n",
    "        look at the docs to see which hyperparameters need tuning.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot feature importance for one of your models (using the `.feature_importances_` property)\n",
    "    - Hint: load `feature_importances_` into a `pandas.Series` and use `.plot(kind='barh')` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Plot a learning curve for the same model\n",
    "    - Hint: This one is tough! Remember that a learning curve is the cross_validation score of a model on the training and test sets as the model learns more and more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your Code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
